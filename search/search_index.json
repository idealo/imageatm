{"config":{"lang":["en"],"prebuild_index":false,"separator":"[\\s\\-]+"},"docs":[{"location":"","text":"Image ATM (Automated Tagging Machine) Image ATM is a one-click tool that automates the workflow of a typical image classification pipeline in an opinionated way, this includes: Preprocessing and validating input images and labels Starting/terminating cloud instance with GPU support Training Model evaluation Read the documentation at: https://idealo.github.io/imageatm/ Image ATM is compatible with Python 3.6 and is distributed under the Apache 2.0 license. Installation There are two ways to install Image ATM: Install Image ATM from PyPI (recommended): pip install imageatm Install Image ATM from the GitHub source: git clone https://github.com/idealo/imageatm.git cd imageatm python setup.py install Usage Train with CLI Run this in your terminal imageatm pipeline config/config_file.yml Train without CLI Run the data preparation: from imageatm.components import DataPrep dp = DataPrep ( samples_file = 'sample_configfile.json' , image_dir = 'sample_dataset/' , job_dir = 'sample_jobdir/' ) dp . run ( resize = True ) Run the training: from imageatm.components import Training trainer = Training ( image_dir = dp . image_dir , job_dir = dp . job_dir ) trainer . run () Run the evaluation: from imageatm.components import Evaluation evaluator = Evaluation ( image_dir = dp . image_dir , job_dir = dp . job_dir ) evaluator . run () Test Test execution is triggered by these commands: pip install -e \".[tests, docs]\" pytest -vs --cov=imageatm --show-capture=no --disable-pytest-warnings tests/ Transfer learning The following pretrained CNNs from Keras can be used for transfer learning in Image-ATM: Xception VGG16 VGG19 ResNet50, ResNet101, ResNet152 ResNet50V2, ResNet101V2, ResNet152V2 ResNeXt50, ResNeXt101 InceptionV3 InceptionResNetV2 MobileNet MobileNetV2 DenseNet121, DenseNet169, DenseNet201 NASNetLarge, NASNetMobile Training is split into two phases, at first only the last dense layer gets trained, and then all layers are trained. For each phase the learning rate is reduced after a patience period if no improvement in validation accuracy has been observed. The patience period depends on the average number of samples per class ( n_per_class ): if n_per_class < 200: patience = 5 epochs if n_per_class >= 200 and < 500: patience = 4 epochs if n_per_class >= 500: patience = 2 epochs Training is stopped early after a patience period that is three times the learning rate patience to allow for two learning rate adjustments before stopping training. Contribute We welcome all kinds of contributions. See the Contribution guide for more details. Bump version To bump up the version, use bumpversion {part} setup.py Cite this work Please cite Image ATM in your publications if this is useful for your research. Here is an example BibTeX entry: @misc{idealods2019imageatm, title={Image ATM}, author={Christopher Lennan and Malgorzata Adamczyk and Gunar Maiwald and Dat Tran}, year={2019}, howpublished={\\url{https://github.com/idealo/imageatm}}, } Maintainers Christopher Lennan, github: clennan Malgorzata Adamczyk, github: gosia-malgosia Gunar Maiwald: github: gunarmaiwald Dat Tran, github: datitran Copyright See LICENSE for details. TO-DOs: We are currently using Keras 2.2. The plan is to use tf.keras once TF 2.0 is out. Currently tf.keras is buggy, especially with model saving/loading (https://github.com/tensorflow/tensorflow/issues/22697)","title":"Home"},{"location":"#image-atm-automated-tagging-machine","text":"Image ATM is a one-click tool that automates the workflow of a typical image classification pipeline in an opinionated way, this includes: Preprocessing and validating input images and labels Starting/terminating cloud instance with GPU support Training Model evaluation Read the documentation at: https://idealo.github.io/imageatm/ Image ATM is compatible with Python 3.6 and is distributed under the Apache 2.0 license.","title":"Image ATM (Automated Tagging Machine)"},{"location":"#installation","text":"There are two ways to install Image ATM: Install Image ATM from PyPI (recommended): pip install imageatm Install Image ATM from the GitHub source: git clone https://github.com/idealo/imageatm.git cd imageatm python setup.py install","title":"Installation"},{"location":"#usage","text":"","title":"Usage"},{"location":"#train-with-cli","text":"Run this in your terminal imageatm pipeline config/config_file.yml","title":"Train with CLI"},{"location":"#train-without-cli","text":"Run the data preparation: from imageatm.components import DataPrep dp = DataPrep ( samples_file = 'sample_configfile.json' , image_dir = 'sample_dataset/' , job_dir = 'sample_jobdir/' ) dp . run ( resize = True ) Run the training: from imageatm.components import Training trainer = Training ( image_dir = dp . image_dir , job_dir = dp . job_dir ) trainer . run () Run the evaluation: from imageatm.components import Evaluation evaluator = Evaluation ( image_dir = dp . image_dir , job_dir = dp . job_dir ) evaluator . run ()","title":"Train without CLI"},{"location":"#test","text":"Test execution is triggered by these commands: pip install -e \".[tests, docs]\" pytest -vs --cov=imageatm --show-capture=no --disable-pytest-warnings tests/","title":"Test"},{"location":"#transfer-learning","text":"The following pretrained CNNs from Keras can be used for transfer learning in Image-ATM: Xception VGG16 VGG19 ResNet50, ResNet101, ResNet152 ResNet50V2, ResNet101V2, ResNet152V2 ResNeXt50, ResNeXt101 InceptionV3 InceptionResNetV2 MobileNet MobileNetV2 DenseNet121, DenseNet169, DenseNet201 NASNetLarge, NASNetMobile Training is split into two phases, at first only the last dense layer gets trained, and then all layers are trained. For each phase the learning rate is reduced after a patience period if no improvement in validation accuracy has been observed. The patience period depends on the average number of samples per class ( n_per_class ): if n_per_class < 200: patience = 5 epochs if n_per_class >= 200 and < 500: patience = 4 epochs if n_per_class >= 500: patience = 2 epochs Training is stopped early after a patience period that is three times the learning rate patience to allow for two learning rate adjustments before stopping training.","title":"Transfer learning"},{"location":"#contribute","text":"We welcome all kinds of contributions. See the Contribution guide for more details.","title":"Contribute"},{"location":"#bump-version","text":"To bump up the version, use bumpversion {part} setup.py","title":"Bump version"},{"location":"#cite-this-work","text":"Please cite Image ATM in your publications if this is useful for your research. Here is an example BibTeX entry: @misc{idealods2019imageatm, title={Image ATM}, author={Christopher Lennan and Malgorzata Adamczyk and Gunar Maiwald and Dat Tran}, year={2019}, howpublished={\\url{https://github.com/idealo/imageatm}}, }","title":"Cite this work"},{"location":"#maintainers","text":"Christopher Lennan, github: clennan Malgorzata Adamczyk, github: gosia-malgosia Gunar Maiwald: github: gunarmaiwald Dat Tran, github: datitran","title":"Maintainers"},{"location":"#copyright","text":"See LICENSE for details.","title":"Copyright"},{"location":"#to-dos","text":"We are currently using Keras 2.2. The plan is to use tf.keras once TF 2.0 is out. Currently tf.keras is buggy, especially with model saving/loading (https://github.com/tensorflow/tensorflow/issues/22697)","title":"TO-DOs:"},{"location":"CONTRIBUTING/","text":"Contribution Guide We welcome any contributions whether it's, Submitting feedback Fixing bugs Or implementing a new feature. Please read this guide before making any contributions. Submit Feedback The feedback should be submitted by creating an issue at GitHub issues . Select the related template (bug report, feature request, or custom) and add the corresponding labels. Fix Bugs: You may look through the GitHub issues for bugs. Implement Features You may look through the GitHub issues for feature requests. Pull Requests (PR) Fork the repository and a create a new branch from the dev branch. For bug fixes, add new tests and for new features please add changes to the documentation. Do a PR from your new branch to our dev branch of the original Image ATM repo. Rules of Engagement Code flow should always go into one direction dev -> master Every PR needs to reference an issue Issues need to be referenced to the Image ATM project when creating it Issues should be referenced to the commits Feature branches should be cloned from latest dev version Assign independent review for merging PR feature branches -> dev Feature branches should be deleted after the merge Every contributor should be able to move dev to master and deploy it to PyPI dev -> master is based on new version bump or decided adhoc on our regular meeting Documentation Make sure any new function or class you introduce has proper docstrings. We use the Google Python Styling Guide for our docstrings. Testing We use pytest for our testing. Make sure to write tests for any new feature and/or bug fixes. Main Contributor List We maintain a list of main contributors to appreciate all the contributions.","title":"Contribution"},{"location":"CONTRIBUTING/#contribution-guide","text":"We welcome any contributions whether it's, Submitting feedback Fixing bugs Or implementing a new feature. Please read this guide before making any contributions.","title":"Contribution Guide"},{"location":"CONTRIBUTING/#submit-feedback","text":"The feedback should be submitted by creating an issue at GitHub issues . Select the related template (bug report, feature request, or custom) and add the corresponding labels.","title":"Submit Feedback"},{"location":"CONTRIBUTING/#fix-bugs","text":"You may look through the GitHub issues for bugs.","title":"Fix Bugs:"},{"location":"CONTRIBUTING/#implement-features","text":"You may look through the GitHub issues for feature requests.","title":"Implement Features"},{"location":"CONTRIBUTING/#pull-requests-pr","text":"Fork the repository and a create a new branch from the dev branch. For bug fixes, add new tests and for new features please add changes to the documentation. Do a PR from your new branch to our dev branch of the original Image ATM repo.","title":"Pull Requests (PR)"},{"location":"CONTRIBUTING/#rules-of-engagement","text":"Code flow should always go into one direction dev -> master Every PR needs to reference an issue Issues need to be referenced to the Image ATM project when creating it Issues should be referenced to the commits Feature branches should be cloned from latest dev version Assign independent review for merging PR feature branches -> dev Feature branches should be deleted after the merge Every contributor should be able to move dev to master and deploy it to PyPI dev -> master is based on new version bump or decided adhoc on our regular meeting","title":"Rules of Engagement"},{"location":"CONTRIBUTING/#documentation","text":"Make sure any new function or class you introduce has proper docstrings. We use the Google Python Styling Guide for our docstrings.","title":"Documentation"},{"location":"CONTRIBUTING/#testing","text":"We use pytest for our testing. Make sure to write tests for any new feature and/or bug fixes.","title":"Testing"},{"location":"CONTRIBUTING/#main-contributor-list","text":"We maintain a list of main contributors to appreciate all the contributions.","title":"Main Contributor List"},{"location":"LICENSE/","text":"Copyright 2019 idealo internet GmbH. All rights reserved. Apache License Version 2.0, January 2004 http://www.apache.org/licenses/ TERMS AND CONDITIONS FOR USE, REPRODUCTION, AND DISTRIBUTION Definitions. \"License\" shall mean the terms and conditions for use, reproduction, and distribution as defined by Sections 1 through 9 of this document. \"Licensor\" shall mean the copyright owner or entity authorized by the copyright owner that is granting the License. \"Legal Entity\" shall mean the union of the acting entity and all other entities that control, are controlled by, or are under common control with that entity. For the purposes of this definition, \"control\" means (i) the power, direct or indirect, to cause the direction or management of such entity, whether by contract or otherwise, or (ii) ownership of fifty percent (50%) or more of the outstanding shares, or (iii) beneficial ownership of such entity. \"You\" (or \"Your\") shall mean an individual or Legal Entity exercising permissions granted by this License. \"Source\" form shall mean the preferred form for making modifications, including but not limited to software source code, documentation source, and configuration files. \"Object\" form shall mean any form resulting from mechanical transformation or translation of a Source form, including but not limited to compiled object code, generated documentation, and conversions to other media types. \"Work\" shall mean the work of authorship, whether in Source or Object form, made available under the License, as indicated by a copyright notice that is included in or attached to the work (an example is provided in the Appendix below). \"Derivative Works\" shall mean any work, whether in Source or Object form, that is based on (or derived from) the Work and for which the editorial revisions, annotations, elaborations, or other modifications represent, as a whole, an original work of authorship. For the purposes of this License, Derivative Works shall not include works that remain separable from, or merely link (or bind by name) to the interfaces of, the Work and Derivative Works thereof. \"Contribution\" shall mean any work of authorship, including the original version of the Work and any modifications or additions to that Work or Derivative Works thereof, that is intentionally submitted to Licensor for inclusion in the Work by the copyright owner or by an individual or Legal Entity authorized to submit on behalf of the copyright owner. For the purposes of this definition, \"submitted\" means any form of electronic, verbal, or written communication sent to the Licensor or its representatives, including but not limited to communication on electronic mailing lists, source code control systems, and issue tracking systems that are managed by, or on behalf of, the Licensor for the purpose of discussing and improving the Work, but excluding communication that is conspicuously marked or otherwise designated in writing by the copyright owner as \"Not a Contribution.\" \"Contributor\" shall mean Licensor and any individual or Legal Entity on behalf of whom a Contribution has been received by Licensor and subsequently incorporated within the Work. Grant of Copyright License. Subject to the terms and conditions of this License, each Contributor hereby grants to You a perpetual, worldwide, non-exclusive, no-charge, royalty-free, irrevocable copyright license to reproduce, prepare Derivative Works of, publicly display, publicly perform, sublicense, and distribute the Work and such Derivative Works in Source or Object form. Grant of Patent License. Subject to the terms and conditions of this License, each Contributor hereby grants to You a perpetual, worldwide, non-exclusive, no-charge, royalty-free, irrevocable (except as stated in this section) patent license to make, have made, use, offer to sell, sell, import, and otherwise transfer the Work, where such license applies only to those patent claims licensable by such Contributor that are necessarily infringed by their Contribution(s) alone or by combination of their Contribution(s) with the Work to which such Contribution(s) was submitted. If You institute patent litigation against any entity (including a cross-claim or counterclaim in a lawsuit) alleging that the Work or a Contribution incorporated within the Work constitutes direct or contributory patent infringement, then any patent licenses granted to You under this License for that Work shall terminate as of the date such litigation is filed. Redistribution. You may reproduce and distribute copies of the Work or Derivative Works thereof in any medium, with or without modifications, and in Source or Object form, provided that You meet the following conditions: (a) You must give any other recipients of the Work or Derivative Works a copy of this License; and (b) You must cause any modified files to carry prominent notices stating that You changed the files; and (c) You must retain, in the Source form of any Derivative Works that You distribute, all copyright, patent, trademark, and attribution notices from the Source form of the Work, excluding those notices that do not pertain to any part of the Derivative Works; and (d) If the Work includes a \"NOTICE\" text file as part of its distribution, then any Derivative Works that You distribute must include a readable copy of the attribution notices contained within such NOTICE file, excluding those notices that do not pertain to any part of the Derivative Works, in at least one of the following places: within a NOTICE text file distributed as part of the Derivative Works; within the Source form or documentation, if provided along with the Derivative Works; or, within a display generated by the Derivative Works, if and wherever such third-party notices normally appear. The contents of the NOTICE file are for informational purposes only and do not modify the License. You may add Your own attribution notices within Derivative Works that You distribute, alongside or as an addendum to the NOTICE text from the Work, provided that such additional attribution notices cannot be construed as modifying the License. You may add Your own copyright statement to Your modifications and may provide additional or different license terms and conditions for use, reproduction, or distribution of Your modifications, or for any such Derivative Works as a whole, provided Your use, reproduction, and distribution of the Work otherwise complies with the conditions stated in this License. Submission of Contributions. Unless You explicitly state otherwise, any Contribution intentionally submitted for inclusion in the Work by You to the Licensor shall be under the terms and conditions of this License, without any additional terms or conditions. Notwithstanding the above, nothing herein shall supersede or modify the terms of any separate license agreement you may have executed with Licensor regarding such Contributions. Trademarks. This License does not grant permission to use the trade names, trademarks, service marks, or product names of the Licensor, except as required for reasonable and customary use in describing the origin of the Work and reproducing the content of the NOTICE file. Disclaimer of Warranty. Unless required by applicable law or agreed to in writing, Licensor provides the Work (and each Contributor provides its Contributions) on an \"AS IS\" BASIS, WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied, including, without limitation, any warranties or conditions of TITLE, NON-INFRINGEMENT, MERCHANTABILITY, or FITNESS FOR A PARTICULAR PURPOSE. You are solely responsible for determining the appropriateness of using or redistributing the Work and assume any risks associated with Your exercise of permissions under this License. Limitation of Liability. In no event and under no legal theory, whether in tort (including negligence), contract, or otherwise, unless required by applicable law (such as deliberate and grossly negligent acts) or agreed to in writing, shall any Contributor be liable to You for damages, including any direct, indirect, special, incidental, or consequential damages of any character arising as a result of this License or out of the use or inability to use the Work (including but not limited to damages for loss of goodwill, work stoppage, computer failure or malfunction, or any and all other commercial damages or losses), even if such Contributor has been advised of the possibility of such damages. Accepting Warranty or Additional Liability. While redistributing the Work or Derivative Works thereof, You may choose to offer, and charge a fee for, acceptance of support, warranty, indemnity, or other liability obligations and/or rights consistent with this License. However, in accepting such obligations, You may act only on Your own behalf and on Your sole responsibility, not on behalf of any other Contributor, and only if You agree to indemnify, defend, and hold each Contributor harmless for any liability incurred by, or claims asserted against, such Contributor by reason of your accepting any such warranty or additional liability. END OF TERMS AND CONDITIONS APPENDIX: How to apply the Apache License to your work. To apply the Apache License to your work, attach the following boilerplate notice, with the fields enclosed by brackets \"[]\" replaced with your own identifying information. (Don't include the brackets!) The text should be enclosed in the appropriate comment syntax for the file format. We also recommend that a file or class name and description of purpose be included on the same \"printed page\" as the copyright notice for easier identification within third-party archives. Copyright [yyyy] [name of copyright owner] Licensed under the Apache License, Version 2.0 (the \"License\"); you may not use this file except in compliance with the License. You may obtain a copy of the License at http://www.apache.org/licenses/LICENSE-2.0 Unless required by applicable law or agreed to in writing, software distributed under the License is distributed on an \"AS IS\" BASIS, WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied. See the License for the specific language governing permissions and limitations under the License.","title":"License"},{"location":"start/","text":"Getting Started Installation Latest Stable Version (pip installation): pip install imageatm Bleeding Edge Version (manual installation): git clone https://github.com/idealo/imageatm.git cd imageatm python setup.py install Folder structure and file formats The starting folder structure is root \u251c\u2500\u2500 config_file.yml \u251c\u2500\u2500 data.json \u2514\u2500\u2500 images \u251c\u2500\u2500 image_1.jpg \u251c\u2500\u2500 image_2.jpg \u251c\u2500\u2500 image_3.jpg \u2514\u2500\u2500 image_4.jpg data.json is a file containing a mapping between the images and their labels. This file must be in the following format: [ { \"image_id\" : \"image_1.jpg\" , \"label\" : \"Class 1\" }, { \"image_id\" : \"image_2.jpg\" , \"label\" : \"Class 1\" }, { \"image_id\" : \"image_3.jpg\" , \"label\" : \"Class 2\" }, { \"image_id\" : \"image_4.jpg\" , \"label\" : \"Class 2\" }, ... ] In the next sections we will use the cats and dogs dataset to showcase all examples. Therefore our starting structure will look as follows: root \u251c\u2500\u2500 data.json \u251c\u2500\u2500 cats_and_dogs_job_dir \u2514\u2500\u2500 cats_and_dogs \u2514\u2500\u2500 train \u251c\u2500\u2500 cat.0.jpg \u251c\u2500\u2500 cat.1.jpg \u251c\u2500\u2500 dog.0.jpg \u2514\u2500\u2500 dog.1.jpg Here you can download the cats and dogs dataset: wget --no-check-certificate \\ https://storage.googleapis.com/mledu-datasets/cats_and_dogs_filtered.zip \\ -O cats_and_dogs_filtered.zip unzip cats_and_dogs_filtered.zip mkdir -p cats_and_dogs/train mv cats_and_dogs_filtered/train/cats/* cats_and_dogs/train mv cats_and_dogs_filtered/train/dogs/* cats_and_dogs/train Convert the data into our input format ( data.json ): [ { \"image_id\" : \"cat.0.jpg\" , \"label\" : \"Cat\" }, { \"image_id\" : \"cat.1.jpg\" , \"label\" : \"Cat\" }, { \"image_id\" : \"dog.0.jpg\" , \"label\" : \"Dog\" }, { \"image_id\" : \"dog.1.jpg\" , \"label\" : \"Dog\" }, ... ] You can use this code to create the data.json : import os import json filenames = os . listdir ( 'cats_and_dogs/train' ) sample_json = [] for i in filenames : sample_json . append ( { 'image_id' : i , 'label' : 'Cat' if 'cat' in i else 'Dog' } ) with open ( 'data.json' , 'w' ) as outfile : json . dump ( sample_json , outfile , indent = 4 , sort_keys = True ) Simple Example for local training Train with CLI Define your config_file.yml : image_dir : cats_and_dogs/train/ job_dir : cats_and_dogs_job_dir/ dataprep : run : True samples_file : data.json resize : True train : run : True evaluate : run : True report : create : True kernel_name : imageatm export_html : True export_pdf : True These configurations will run three components in a pipeline: data preparation, training and evaluation. In case you set create: True a jupyter notebook is created. Be sure to set a proper kernel_name . If you don't have an IPython ernel available create one . In case you like to export report to html set export_html: True . In case you like to export report to pdf set export_pdf: True . This will be done via Latex. Be sure to have installed all required packages for Latex. Then run: imageatm pipeline config/config_file.yml The resulting folder structure will look like this root \u251c\u2500\u2500 config_file.yml \u251c\u2500\u2500 data.json \u251c\u2500\u2500 cats_and_dogs \u2502 \u2514\u2500\u2500 train \u2502 \u251c\u2500\u2500 cat.0.jpg \u2502 \u251c\u2500\u2500 cat.1.jpg \u2502 \u251c\u2500\u2500 dog.0.jpg \u2502 \u2514\u2500\u2500 dog.1.jpg \u2514\u2500\u2500 job_dirs \u2514\u2500\u2500 cats_and_dogs_job_dir \u251c\u2500\u2500 class_mapping.json \u251c\u2500\u2500 test_samples.json \u251c\u2500\u2500 train_samples.json \u251c\u2500\u2500 val_samples.json \u251c\u2500\u2500 logs \u251c\u2500\u2500 models \u2502 \u251c\u2500\u2500 model_1.hdf5 \u2502 \u251c\u2500\u2500 model_2.hdf5 \u2502 \u2514\u2500\u2500 model_3.hdf5 \u2514\u2500\u2500 evaluation \u251c\u2500\u2500 evaluation_report.html \u251c\u2500\u2500 evaluation_report.ipynb \u2514\u2500\u2500 evaluation_report.pdf Train without CLI Run the data preparation: from imageatm.components import DataPrep dp = DataPrep ( image_dir = 'cats_and_dogs/train' , samples_file = 'data.json' , job_dir = 'cats_and_dogs_job_dir' ) dp . run ( resize = True ) Run the training: from imageatm.components import Training trainer = Training ( dp . image_dir , dp . job_dir ) trainer . run () Run the evaluation: from imageatm.components import Evaluation evaluater = Evaluation ( image_dir = dp . image_dir , job_dir = dp . job_dir ) evaluater . run () Simple Example for cloud training Initial cloud set-up To train your model using cloud services you'll need an existing S3 bucket where you will be able to store the content of your local job_dir and image_dir as well as trained models. If you don't have an S3 bucket you'll have to create one . Assign the following IAM roles to your AWS user account iam role: { \"Version\" : \"2012-10-17\" , \"Statement\" : [ { \"Sid\" : \"Stmt1508403745000\" , \"Effect\" : \"Allow\" , \"Action\" : [ \"iam:CreatePolicy\" , \"iam:CreateRole\" , \"iam:GetPolicy\" , \"iam:GetRole\" , \"iam:GetPolicyVersion\" , \"iam:CreateInstanceProfile\" , \"iam:AttachRolePolicy\" , \"iam:ListRolePolicies\" , \"iam:GetInstanceProfile\" , \"iam:ListEntitiesForPolicy\" , \"iam:ListPolicyVersions\" , \"iam:CreatePolicyVersion\" , \"iam:RemoveRoleFromInstanceProfile\" , \"iam:DetachRolePolicy\" , \"iam:DeleteInstanceProfile\" , \"iam:DeletePolicyVersion\" , \"iam:ListInstanceProfilesForRole\" , \"iam:DeletePolicy\" , \"iam:DeleteRole\" , \"iam:AddRoleToInstanceProfile\" , \"iam:PassRole\" ], \"Resource\" : [ \"*\" ] } ] } ec2 role: { \"Version\" : \"2012-10-17\" , \"Statement\" : [ { \"Action\" : \"ec2:*\" , \"Effect\" : \"Allow\" , \"Resource\" : \"*\" }, { \"Effect\" : \"Allow\" , \"Action\" : \"elasticloadbalancing:*\" , \"Resource\" : \"*\" }, { \"Effect\" : \"Allow\" , \"Action\" : \"cloudwatch:*\" , \"Resource\" : \"*\" }, { \"Effect\" : \"Allow\" , \"Action\" : \"autoscaling:*\" , \"Resource\" : \"*\" }, { \"Effect\" : \"Allow\" , \"Action\" : \"iam:CreateServiceLinkedRole\" , \"Resource\" : \"*\" , \"Condition\" : { \"StringEquals\" : { \"iam:AWSServiceName\" : [ \"autoscaling.amazonaws.com\" , \"ec2scheduled.amazonaws.com\" , \"elasticloadbalancing.amazonaws.com\" , \"spot.amazonaws.com\" , \"spotfleet.amazonaws.com\" , \"transitgateway.amazonaws.com\" ] } } } ] } s3 role: { \"Version\" : \"2012-10-17\" , \"Statement\" : [ { \"Effect\" : \"Allow\" , \"Action\" : \"s3:*\" , \"Resource\" : \"*\" } ] } Configure your AWS client In your shell run aws configure and input: - AWS Access Key ID - AWS Secret Access Key - Default region name ( for example `eu-central-1` ) - Default output format ( for example `json` ) Now you are ready to kick off with the cloud training. Train with CLI Define your config_file.yml : image_dir : cats_and_dogs/train job_dir : cats_and_dogs_job_dir/ dataprep : run : True samples_file : data.json resize : True train : run : True cloud : True cloud : cloud_tag : image_atm_example_tag provider : aws tf_dir : cloud/aws region : eu-west-1 vpc_id : vpc-example instance_type : p2.xlarge bucket : s3://example-bucket destroy : True These configurations will locally run data preparation, then launch an AWS EC2 instance, run a training on it and copy the content of your local job_dir and image_dir to the pre-defined S3 bucket. As we set destroy: True in this example, it will also destroy the EC2 instance and all dependencies once the training finished. If you want to reuse the instance for multiple experiments, set destroy: False . Run imageatm in shell: imageatm pipeline config/config_file.yml The resulting S3 bucket structure will look like this: s3://example-bucket \u251c\u2500\u2500 image_dirs \u2502 \u2514\u2500\u2500 train_resized \u2502 \u251c\u2500\u2500 cat.0.jpg \u2502 \u251c\u2500\u2500 cat.1.jpg \u2502 \u251c\u2500\u2500 dog.0.jpg \u2502 \u2514\u2500\u2500 dog.1.jpg \u2514\u2500\u2500 job_dirs \u2514\u2500\u2500 cats_and_dogs_job_dir \u251c\u2500\u2500 class_mapping.json \u251c\u2500\u2500 test_samples.json \u251c\u2500\u2500 train_samples.json \u251c\u2500\u2500 val_samples.json \u2514\u2500\u2500 models \u251c\u2500\u2500 model_1.hdf5 \u251c\u2500\u2500 model_2.hdf5 \u2514\u2500\u2500 model_3.hdf5 The resulting local structure will look like this: root \u251c\u2500\u2500 cats_and_dogs \u2502 \u251c\u2500\u2500 train \u2502 \u2502 \u251c\u2500\u2500 cat.0.jpg \u2502 \u2502 \u251c\u2500\u2500 cat.1.jpg \u2502 \u2502 \u251c\u2500\u2500 dog.0.jpg \u2502 \u2502 \u251c\u2500\u2500 dog.1.jpg \u2502 \u2514\u2500\u2500 train_resized \u2502 \u251c\u2500\u2500 cat.0.jpg \u2502 \u251c\u2500\u2500 cat.1.jpg \u2502 \u251c\u2500\u2500 dog.0.jpg \u2502 \u2514\u2500\u2500 dog.1.jpg \u2502 \u2514\u2500\u2500 cats_and_dogs_job_dir \u251c\u2500\u2500 class_mapping.json \u251c\u2500\u2500 test_samples.json \u251c\u2500\u2500 train_samples.json \u251c\u2500\u2500 val_samples.json \u251c\u2500\u2500 logs \u2514\u2500\u2500 models \u251c\u2500\u2500 model_1.hdf5 \u251c\u2500\u2500 model_2.hdf5 \u2514\u2500\u2500 model_3.hdf5 Train without CLI Make sure you've got your cloud setup ready as described in the cloud training introduction . Run the data preparation: from imageatm.components import DataPrep dp = DataPrep ( image_dir = 'cats_and_dogs/train' , samples_file = 'data.json' , job_dir = 'cats_and_dogs_job_dir' ) dp . run ( resize = True ) Run the training: from imageatm.components import AWS cloud = AWS ( tf_dir = 'cloud/aws' , region = 'eu-west-1' , instance_type = 'p2.xlarge' , vpc_id = 'vpc-example' , s3_bucket = 's3://example-bucket' , job_dir = dp . job_dir , cloud_tag = 'image_atm_example_tag' , ) cloud . init () cloud . apply () cloud . train ( image_dir = dp . image_dir ) Once the training is completed you have to manually destroy the AWS instance and its dependencies: cloud . destroy ()","title":"Getting Started"},{"location":"start/#getting-started","text":"","title":"Getting Started"},{"location":"start/#installation","text":"","title":"Installation"},{"location":"start/#latest-stable-version-pip-installation","text":"pip install imageatm","title":"Latest Stable Version (pip installation):"},{"location":"start/#bleeding-edge-version-manual-installation","text":"git clone https://github.com/idealo/imageatm.git cd imageatm python setup.py install","title":"Bleeding Edge Version (manual installation):"},{"location":"start/#folder-structure-and-file-formats","text":"The starting folder structure is root \u251c\u2500\u2500 config_file.yml \u251c\u2500\u2500 data.json \u2514\u2500\u2500 images \u251c\u2500\u2500 image_1.jpg \u251c\u2500\u2500 image_2.jpg \u251c\u2500\u2500 image_3.jpg \u2514\u2500\u2500 image_4.jpg data.json is a file containing a mapping between the images and their labels. This file must be in the following format: [ { \"image_id\" : \"image_1.jpg\" , \"label\" : \"Class 1\" }, { \"image_id\" : \"image_2.jpg\" , \"label\" : \"Class 1\" }, { \"image_id\" : \"image_3.jpg\" , \"label\" : \"Class 2\" }, { \"image_id\" : \"image_4.jpg\" , \"label\" : \"Class 2\" }, ... ] In the next sections we will use the cats and dogs dataset to showcase all examples. Therefore our starting structure will look as follows: root \u251c\u2500\u2500 data.json \u251c\u2500\u2500 cats_and_dogs_job_dir \u2514\u2500\u2500 cats_and_dogs \u2514\u2500\u2500 train \u251c\u2500\u2500 cat.0.jpg \u251c\u2500\u2500 cat.1.jpg \u251c\u2500\u2500 dog.0.jpg \u2514\u2500\u2500 dog.1.jpg Here you can download the cats and dogs dataset: wget --no-check-certificate \\ https://storage.googleapis.com/mledu-datasets/cats_and_dogs_filtered.zip \\ -O cats_and_dogs_filtered.zip unzip cats_and_dogs_filtered.zip mkdir -p cats_and_dogs/train mv cats_and_dogs_filtered/train/cats/* cats_and_dogs/train mv cats_and_dogs_filtered/train/dogs/* cats_and_dogs/train Convert the data into our input format ( data.json ): [ { \"image_id\" : \"cat.0.jpg\" , \"label\" : \"Cat\" }, { \"image_id\" : \"cat.1.jpg\" , \"label\" : \"Cat\" }, { \"image_id\" : \"dog.0.jpg\" , \"label\" : \"Dog\" }, { \"image_id\" : \"dog.1.jpg\" , \"label\" : \"Dog\" }, ... ] You can use this code to create the data.json : import os import json filenames = os . listdir ( 'cats_and_dogs/train' ) sample_json = [] for i in filenames : sample_json . append ( { 'image_id' : i , 'label' : 'Cat' if 'cat' in i else 'Dog' } ) with open ( 'data.json' , 'w' ) as outfile : json . dump ( sample_json , outfile , indent = 4 , sort_keys = True )","title":"Folder structure and file formats"},{"location":"start/#simple-example-for-local-training","text":"","title":"Simple Example for local training"},{"location":"start/#train-with-cli","text":"Define your config_file.yml : image_dir : cats_and_dogs/train/ job_dir : cats_and_dogs_job_dir/ dataprep : run : True samples_file : data.json resize : True train : run : True evaluate : run : True report : create : True kernel_name : imageatm export_html : True export_pdf : True These configurations will run three components in a pipeline: data preparation, training and evaluation. In case you set create: True a jupyter notebook is created. Be sure to set a proper kernel_name . If you don't have an IPython ernel available create one . In case you like to export report to html set export_html: True . In case you like to export report to pdf set export_pdf: True . This will be done via Latex. Be sure to have installed all required packages for Latex. Then run: imageatm pipeline config/config_file.yml The resulting folder structure will look like this root \u251c\u2500\u2500 config_file.yml \u251c\u2500\u2500 data.json \u251c\u2500\u2500 cats_and_dogs \u2502 \u2514\u2500\u2500 train \u2502 \u251c\u2500\u2500 cat.0.jpg \u2502 \u251c\u2500\u2500 cat.1.jpg \u2502 \u251c\u2500\u2500 dog.0.jpg \u2502 \u2514\u2500\u2500 dog.1.jpg \u2514\u2500\u2500 job_dirs \u2514\u2500\u2500 cats_and_dogs_job_dir \u251c\u2500\u2500 class_mapping.json \u251c\u2500\u2500 test_samples.json \u251c\u2500\u2500 train_samples.json \u251c\u2500\u2500 val_samples.json \u251c\u2500\u2500 logs \u251c\u2500\u2500 models \u2502 \u251c\u2500\u2500 model_1.hdf5 \u2502 \u251c\u2500\u2500 model_2.hdf5 \u2502 \u2514\u2500\u2500 model_3.hdf5 \u2514\u2500\u2500 evaluation \u251c\u2500\u2500 evaluation_report.html \u251c\u2500\u2500 evaluation_report.ipynb \u2514\u2500\u2500 evaluation_report.pdf","title":"Train with CLI"},{"location":"start/#train-without-cli","text":"Run the data preparation: from imageatm.components import DataPrep dp = DataPrep ( image_dir = 'cats_and_dogs/train' , samples_file = 'data.json' , job_dir = 'cats_and_dogs_job_dir' ) dp . run ( resize = True ) Run the training: from imageatm.components import Training trainer = Training ( dp . image_dir , dp . job_dir ) trainer . run () Run the evaluation: from imageatm.components import Evaluation evaluater = Evaluation ( image_dir = dp . image_dir , job_dir = dp . job_dir ) evaluater . run ()","title":"Train without CLI"},{"location":"start/#simple-example-for-cloud-training","text":"","title":"Simple Example for cloud training"},{"location":"start/#initial-cloud-set-up","text":"To train your model using cloud services you'll need an existing S3 bucket where you will be able to store the content of your local job_dir and image_dir as well as trained models. If you don't have an S3 bucket you'll have to create one .","title":"Initial cloud set-up"},{"location":"start/#assign-the-following-iam-roles-to-your-aws-user-account","text":"iam role: { \"Version\" : \"2012-10-17\" , \"Statement\" : [ { \"Sid\" : \"Stmt1508403745000\" , \"Effect\" : \"Allow\" , \"Action\" : [ \"iam:CreatePolicy\" , \"iam:CreateRole\" , \"iam:GetPolicy\" , \"iam:GetRole\" , \"iam:GetPolicyVersion\" , \"iam:CreateInstanceProfile\" , \"iam:AttachRolePolicy\" , \"iam:ListRolePolicies\" , \"iam:GetInstanceProfile\" , \"iam:ListEntitiesForPolicy\" , \"iam:ListPolicyVersions\" , \"iam:CreatePolicyVersion\" , \"iam:RemoveRoleFromInstanceProfile\" , \"iam:DetachRolePolicy\" , \"iam:DeleteInstanceProfile\" , \"iam:DeletePolicyVersion\" , \"iam:ListInstanceProfilesForRole\" , \"iam:DeletePolicy\" , \"iam:DeleteRole\" , \"iam:AddRoleToInstanceProfile\" , \"iam:PassRole\" ], \"Resource\" : [ \"*\" ] } ] } ec2 role: { \"Version\" : \"2012-10-17\" , \"Statement\" : [ { \"Action\" : \"ec2:*\" , \"Effect\" : \"Allow\" , \"Resource\" : \"*\" }, { \"Effect\" : \"Allow\" , \"Action\" : \"elasticloadbalancing:*\" , \"Resource\" : \"*\" }, { \"Effect\" : \"Allow\" , \"Action\" : \"cloudwatch:*\" , \"Resource\" : \"*\" }, { \"Effect\" : \"Allow\" , \"Action\" : \"autoscaling:*\" , \"Resource\" : \"*\" }, { \"Effect\" : \"Allow\" , \"Action\" : \"iam:CreateServiceLinkedRole\" , \"Resource\" : \"*\" , \"Condition\" : { \"StringEquals\" : { \"iam:AWSServiceName\" : [ \"autoscaling.amazonaws.com\" , \"ec2scheduled.amazonaws.com\" , \"elasticloadbalancing.amazonaws.com\" , \"spot.amazonaws.com\" , \"spotfleet.amazonaws.com\" , \"transitgateway.amazonaws.com\" ] } } } ] } s3 role: { \"Version\" : \"2012-10-17\" , \"Statement\" : [ { \"Effect\" : \"Allow\" , \"Action\" : \"s3:*\" , \"Resource\" : \"*\" } ] }","title":"Assign the following IAM roles to your AWS user account"},{"location":"start/#configure-your-aws-client","text":"In your shell run aws configure and input: - AWS Access Key ID - AWS Secret Access Key - Default region name ( for example `eu-central-1` ) - Default output format ( for example `json` ) Now you are ready to kick off with the cloud training.","title":"Configure your AWS client"},{"location":"start/#train-with-cli_1","text":"Define your config_file.yml : image_dir : cats_and_dogs/train job_dir : cats_and_dogs_job_dir/ dataprep : run : True samples_file : data.json resize : True train : run : True cloud : True cloud : cloud_tag : image_atm_example_tag provider : aws tf_dir : cloud/aws region : eu-west-1 vpc_id : vpc-example instance_type : p2.xlarge bucket : s3://example-bucket destroy : True These configurations will locally run data preparation, then launch an AWS EC2 instance, run a training on it and copy the content of your local job_dir and image_dir to the pre-defined S3 bucket. As we set destroy: True in this example, it will also destroy the EC2 instance and all dependencies once the training finished. If you want to reuse the instance for multiple experiments, set destroy: False . Run imageatm in shell: imageatm pipeline config/config_file.yml The resulting S3 bucket structure will look like this: s3://example-bucket \u251c\u2500\u2500 image_dirs \u2502 \u2514\u2500\u2500 train_resized \u2502 \u251c\u2500\u2500 cat.0.jpg \u2502 \u251c\u2500\u2500 cat.1.jpg \u2502 \u251c\u2500\u2500 dog.0.jpg \u2502 \u2514\u2500\u2500 dog.1.jpg \u2514\u2500\u2500 job_dirs \u2514\u2500\u2500 cats_and_dogs_job_dir \u251c\u2500\u2500 class_mapping.json \u251c\u2500\u2500 test_samples.json \u251c\u2500\u2500 train_samples.json \u251c\u2500\u2500 val_samples.json \u2514\u2500\u2500 models \u251c\u2500\u2500 model_1.hdf5 \u251c\u2500\u2500 model_2.hdf5 \u2514\u2500\u2500 model_3.hdf5 The resulting local structure will look like this: root \u251c\u2500\u2500 cats_and_dogs \u2502 \u251c\u2500\u2500 train \u2502 \u2502 \u251c\u2500\u2500 cat.0.jpg \u2502 \u2502 \u251c\u2500\u2500 cat.1.jpg \u2502 \u2502 \u251c\u2500\u2500 dog.0.jpg \u2502 \u2502 \u251c\u2500\u2500 dog.1.jpg \u2502 \u2514\u2500\u2500 train_resized \u2502 \u251c\u2500\u2500 cat.0.jpg \u2502 \u251c\u2500\u2500 cat.1.jpg \u2502 \u251c\u2500\u2500 dog.0.jpg \u2502 \u2514\u2500\u2500 dog.1.jpg \u2502 \u2514\u2500\u2500 cats_and_dogs_job_dir \u251c\u2500\u2500 class_mapping.json \u251c\u2500\u2500 test_samples.json \u251c\u2500\u2500 train_samples.json \u251c\u2500\u2500 val_samples.json \u251c\u2500\u2500 logs \u2514\u2500\u2500 models \u251c\u2500\u2500 model_1.hdf5 \u251c\u2500\u2500 model_2.hdf5 \u2514\u2500\u2500 model_3.hdf5","title":"Train with CLI"},{"location":"start/#train-without-cli_1","text":"Make sure you've got your cloud setup ready as described in the cloud training introduction . Run the data preparation: from imageatm.components import DataPrep dp = DataPrep ( image_dir = 'cats_and_dogs/train' , samples_file = 'data.json' , job_dir = 'cats_and_dogs_job_dir' ) dp . run ( resize = True ) Run the training: from imageatm.components import AWS cloud = AWS ( tf_dir = 'cloud/aws' , region = 'eu-west-1' , instance_type = 'p2.xlarge' , vpc_id = 'vpc-example' , s3_bucket = 's3://example-bucket' , job_dir = dp . job_dir , cloud_tag = 'image_atm_example_tag' , ) cloud . init () cloud . apply () cloud . train ( image_dir = dp . image_dir ) Once the training is completed you have to manually destroy the AWS instance and its dependencies: cloud . destroy ()","title":"Train without CLI"},{"location":"client/client/","text":"cli def cli ( config ) pipeline def pipeline ( config , ** kwargs ) Runs all components for which run=True in config file. All activated (run=True) components from config file will be run in sequence. Options overwrite the config file. The config file is the only way to define pipeline components. Args config-file : Central configuration file. dataprep def dataprep ( config , ** kwargs ) Run data preparation and create job dir. Creates a directory (job_dir) with the following files: train_samples.json val_samples.json test_samples.json class_mapping.json train def train ( config , ** kwargs ) Train a CNN. Fine-tunes an ImageNet pre-trained CNN. The number of classes are derived from train_samples.json. After each epoch the model will be evaluated on val_samples.json. The best model (based on valuation accuracy) will be saved. Args image_dir : Directory with image files. job_dir : Directory with train_samples, val_samples, and class_mapping.json. evaluate def evaluate ( config , ** kwargs ) Evaluate a trained model. Evaluation will be performed on test_samples.json. Args image_dir : Directory with image files. job_dir : Directory with test_samples.json and class_mapping.json. cloud def cloud ( config , ** kwargs ) Launch/destroy a cloud compute instance. Launch/destroy cloud instances with Terraform based on Terraform files in tf_dir.","title":"Client"},{"location":"client/client/#cli","text":"def cli ( config )","title":"cli"},{"location":"client/client/#pipeline","text":"def pipeline ( config , ** kwargs ) Runs all components for which run=True in config file. All activated (run=True) components from config file will be run in sequence. Options overwrite the config file. The config file is the only way to define pipeline components.","title":"pipeline"},{"location":"client/client/#args","text":"config-file : Central configuration file.","title":"Args"},{"location":"client/client/#dataprep","text":"def dataprep ( config , ** kwargs ) Run data preparation and create job dir. Creates a directory (job_dir) with the following files: train_samples.json val_samples.json test_samples.json class_mapping.json","title":"dataprep"},{"location":"client/client/#train","text":"def train ( config , ** kwargs ) Train a CNN. Fine-tunes an ImageNet pre-trained CNN. The number of classes are derived from train_samples.json. After each epoch the model will be evaluated on val_samples.json. The best model (based on valuation accuracy) will be saved.","title":"train"},{"location":"client/client/#args_1","text":"image_dir : Directory with image files. job_dir : Directory with train_samples, val_samples, and class_mapping.json.","title":"Args"},{"location":"client/client/#evaluate","text":"def evaluate ( config , ** kwargs ) Evaluate a trained model. Evaluation will be performed on test_samples.json.","title":"evaluate"},{"location":"client/client/#args_2","text":"image_dir : Directory with image files. job_dir : Directory with test_samples.json and class_mapping.json.","title":"Args"},{"location":"client/client/#cloud","text":"def cloud ( config , ** kwargs ) Launch/destroy a cloud compute instance. Launch/destroy cloud instances with Terraform based on Terraform files in tf_dir.","title":"cloud"},{"location":"client/commands/","text":"pipeline def pipeline ( config , config_file , job_dir , image_dir , samples_file , provider , instance_type , region , vpc_id , bucket , tf_dir , train_cloud , destroy , resize , batch_size , learning_rate_dense , learning_rate_all , epochs_train_dense , epochs_train_all , base_model_name , cloud_tag , create_report , kernel_name , export_html , export_pdf ) Runs the entire pipeline based on config file. dataprep def dataprep ( config , config_file , image_dir , samples_file , job_dir , resize ) train def train ( config , config_file , job_dir , image_dir , provider , instance_type , region , vpc_id , bucket , tf_dir , train_cloud , destroy , batch_size , learning_rate_dense , learning_rate_all , epochs_train_dense , epochs_train_all , base_model_name , cloud_tag ) evaluate def evaluate ( config , config_file , image_dir , job_dir , create_report , kernel_name , export_html , export_pdf ) cloud def cloud ( config , job_dir , config_file , provider , instance_type , region , vpc_id , bucket , tf_dir , train_cloud , destroy , no_destroy , cloud_tag )","title":"Commands"},{"location":"client/commands/#pipeline","text":"def pipeline ( config , config_file , job_dir , image_dir , samples_file , provider , instance_type , region , vpc_id , bucket , tf_dir , train_cloud , destroy , resize , batch_size , learning_rate_dense , learning_rate_all , epochs_train_dense , epochs_train_all , base_model_name , cloud_tag , create_report , kernel_name , export_html , export_pdf ) Runs the entire pipeline based on config file.","title":"pipeline"},{"location":"client/commands/#dataprep","text":"def dataprep ( config , config_file , image_dir , samples_file , job_dir , resize )","title":"dataprep"},{"location":"client/commands/#train","text":"def train ( config , config_file , job_dir , image_dir , provider , instance_type , region , vpc_id , bucket , tf_dir , train_cloud , destroy , batch_size , learning_rate_dense , learning_rate_all , epochs_train_dense , epochs_train_all , base_model_name , cloud_tag )","title":"train"},{"location":"client/commands/#evaluate","text":"def evaluate ( config , config_file , image_dir , job_dir , create_report , kernel_name , export_html , export_pdf )","title":"evaluate"},{"location":"client/commands/#cloud","text":"def cloud ( config , job_dir , config_file , provider , instance_type , region , vpc_id , bucket , tf_dir , train_cloud , destroy , no_destroy , cloud_tag )","title":"cloud"},{"location":"client/config/","text":"update_component_configs def update_component_configs ( config ) Populate central parameters to component configs. update_config def update_config ( config , config_file , job_dir , image_dir , samples_file , provider , instance_type , region , vpc_id , bucket , tf_dir , train_cloud , destroy , no_destroy , resize , batch_size , learning_rate_dense , learning_rate_all , epochs_train_dense , epochs_train_all , base_model_name , cloud_tag , create_report , kernel_name , export_html , export_pdf ) get_diff def get_diff ( name , config , required_keys , optional_keys ) val_dataprep def val_dataprep ( config ) val_train def val_train ( config ) val_evaluate def val_evaluate ( config ) val_cloud def val_cloud ( config ) validate_config def validate_config ( config , components ) class Config __init__ def __init__ ()","title":"Config"},{"location":"client/config/#update95component95configs","text":"def update_component_configs ( config ) Populate central parameters to component configs.","title":"update_component_configs"},{"location":"client/config/#update95config","text":"def update_config ( config , config_file , job_dir , image_dir , samples_file , provider , instance_type , region , vpc_id , bucket , tf_dir , train_cloud , destroy , no_destroy , resize , batch_size , learning_rate_dense , learning_rate_all , epochs_train_dense , epochs_train_all , base_model_name , cloud_tag , create_report , kernel_name , export_html , export_pdf )","title":"update_config"},{"location":"client/config/#get95diff","text":"def get_diff ( name , config , required_keys , optional_keys )","title":"get_diff"},{"location":"client/config/#val95dataprep","text":"def val_dataprep ( config )","title":"val_dataprep"},{"location":"client/config/#val95train","text":"def val_train ( config )","title":"val_train"},{"location":"client/config/#val95evaluate","text":"def val_evaluate ( config )","title":"val_evaluate"},{"location":"client/config/#val95cloud","text":"def val_cloud ( config )","title":"val_cloud"},{"location":"client/config/#validate95config","text":"def validate_config ( config , components )","title":"validate_config"},{"location":"client/config/#class-config","text":"","title":"class Config"},{"location":"client/config/#9595init9595","text":"def __init__ ()","title":"__init__"},{"location":"components/cloud/","text":"class AWS Cloud provider class that allows to run training on AWS. Cloud instances are created and destroyed using Terraform. The instance is provisioned with nvidia-docker and training is run in a Docker container using the public Docker image idealo/tensorflow-image-atm:1.13.1 . All commands on EC2 instance will be run via SSH. For training the local image and job directories will be synced with S3. After training the trained models will be synced with S3 and the local job directory. Attributes tf_dir : Directory with Terraform files for AWS setup. region : AWS region [eu-west-1, eu-central-1]. instance_type : AWS GPU instance type [g2.*, p2.*, p3.*]. vpc_id : AWS Virtual Private Cloud ID. s3_bucket : AWS S3 bucket where all training files will be stored (is not created by Terraform). job_dir : Job directory on local system (needed for logging). cloud_tag : Name under which all AWS resources will be set up. __init__ def __init__ ( tf_dir , region , instance_type , vpc_id , s3_bucket , job_dir , cloud_tag , ** kwargs ) Inits cloud component. Sets remote workdir and ensures that s3 bucket prefix is correct. init def init () Runs Terraform initialization. apply def apply () Runs Terraform apply. train def train ( image_dir , job_dir , ** kwargs ) Runs training on EC2 instance. The following steps will be performed in sequence: syncs local image and job directory with S3 syncs S3 with EC2 instance launches Docker training container on EC2 syncs EC2 with S3 syncs S3 with local. Any of the pre-trained CNNs in Keras can be used. Args image_dir : Directory with images used for training. job_dir : Directory with train_samples.json, val_samples.json, and class_mapping.json. epochs_train_dense : Number of epochs to train dense layers. epochs_train_all : Number of epochs to train all layers. learning_rate_dense : Learning rate for dense training phase. self.learning_rate_all : Learning rate for all training phase. batch_size : Number of images per batch. dropout_rate : Fraction set randomly. base_model_name : Name of pretrained CNN. destroy def destroy () Runs Terraform destroy.","title":"Cloud"},{"location":"components/cloud/#class-aws","text":"Cloud provider class that allows to run training on AWS. Cloud instances are created and destroyed using Terraform. The instance is provisioned with nvidia-docker and training is run in a Docker container using the public Docker image idealo/tensorflow-image-atm:1.13.1 . All commands on EC2 instance will be run via SSH. For training the local image and job directories will be synced with S3. After training the trained models will be synced with S3 and the local job directory.","title":"class AWS"},{"location":"components/cloud/#attributes","text":"tf_dir : Directory with Terraform files for AWS setup. region : AWS region [eu-west-1, eu-central-1]. instance_type : AWS GPU instance type [g2.*, p2.*, p3.*]. vpc_id : AWS Virtual Private Cloud ID. s3_bucket : AWS S3 bucket where all training files will be stored (is not created by Terraform). job_dir : Job directory on local system (needed for logging). cloud_tag : Name under which all AWS resources will be set up.","title":"Attributes"},{"location":"components/cloud/#9595init9595","text":"def __init__ ( tf_dir , region , instance_type , vpc_id , s3_bucket , job_dir , cloud_tag , ** kwargs ) Inits cloud component. Sets remote workdir and ensures that s3 bucket prefix is correct.","title":"__init__"},{"location":"components/cloud/#init","text":"def init () Runs Terraform initialization.","title":"init"},{"location":"components/cloud/#apply","text":"def apply () Runs Terraform apply.","title":"apply"},{"location":"components/cloud/#train","text":"def train ( image_dir , job_dir , ** kwargs ) Runs training on EC2 instance. The following steps will be performed in sequence: syncs local image and job directory with S3 syncs S3 with EC2 instance launches Docker training container on EC2 syncs EC2 with S3 syncs S3 with local. Any of the pre-trained CNNs in Keras can be used.","title":"train"},{"location":"components/cloud/#args","text":"image_dir : Directory with images used for training. job_dir : Directory with train_samples.json, val_samples.json, and class_mapping.json. epochs_train_dense : Number of epochs to train dense layers. epochs_train_all : Number of epochs to train all layers. learning_rate_dense : Learning rate for dense training phase. self.learning_rate_all : Learning rate for all training phase. batch_size : Number of images per batch. dropout_rate : Fraction set randomly. base_model_name : Name of pretrained CNN.","title":"Args"},{"location":"components/cloud/#destroy","text":"def destroy () Runs Terraform destroy.","title":"destroy"},{"location":"components/dataprep/","text":"class DataPrep Prepares data for training. Based on a samples file and an image directory a train, validation, and test set will be created. Before data preparation starts samples file and image directory will be validated. DataPrep creates a job directory with the following set of files class_mapping.json train_samples.json val_samples.json test_samples.json These files are required for subsequent components (training and evaluation). The samples file must be in JSON format with the following keys [ { \"image_id\": \"image_1.jpg\", \"label\": \"Class 1\" }, { \"image_id\": \"image_2.jpg\", \"label\": \"Class 1\" }, ... ] Attributes image_dir : path of image directory. job_dir : path to job directory with samples. samples_file : path to samples file. min_class_size : minimal number of samples per label (default 2). test_size : represent the proportion of the dataset to include in the test set (default 0.2). val_size : represent the proportion of the dataset to include in the val set (default 0.1). part_size : represent the proportion of the dataset to include in all sets (default 1). __init__ def __init__ ( job_dir , image_dir , samples_file , min_class_size , test_size , val_size , part_size , ** kwargs ) Inits data preparation component. Loads samples file. Initializes variables for further operations: valid_image_ids , class_mapping , train_samples , val_samples , test_samples . run def run ( resize ) Executes all steps of data preparation. Validates samples and images Creates class-mapping (string to integer) Applies class-mapping on samples Splits sample into train, validation and test sets Resizes images Saves files (class-mapping, train-, validation- and test-set) Args resize : boolean (creates a subfolder of resized images, default False).","title":"Data Preparation"},{"location":"components/dataprep/#class-dataprep","text":"Prepares data for training. Based on a samples file and an image directory a train, validation, and test set will be created. Before data preparation starts samples file and image directory will be validated. DataPrep creates a job directory with the following set of files class_mapping.json train_samples.json val_samples.json test_samples.json These files are required for subsequent components (training and evaluation). The samples file must be in JSON format with the following keys [ { \"image_id\": \"image_1.jpg\", \"label\": \"Class 1\" }, { \"image_id\": \"image_2.jpg\", \"label\": \"Class 1\" }, ... ]","title":"class DataPrep"},{"location":"components/dataprep/#attributes","text":"image_dir : path of image directory. job_dir : path to job directory with samples. samples_file : path to samples file. min_class_size : minimal number of samples per label (default 2). test_size : represent the proportion of the dataset to include in the test set (default 0.2). val_size : represent the proportion of the dataset to include in the val set (default 0.1). part_size : represent the proportion of the dataset to include in all sets (default 1).","title":"Attributes"},{"location":"components/dataprep/#9595init9595","text":"def __init__ ( job_dir , image_dir , samples_file , min_class_size , test_size , val_size , part_size , ** kwargs ) Inits data preparation component. Loads samples file. Initializes variables for further operations: valid_image_ids , class_mapping , train_samples , val_samples , test_samples .","title":"__init__"},{"location":"components/dataprep/#run","text":"def run ( resize ) Executes all steps of data preparation. Validates samples and images Creates class-mapping (string to integer) Applies class-mapping on samples Splits sample into train, validation and test sets Resizes images Saves files (class-mapping, train-, validation- and test-set)","title":"run"},{"location":"components/dataprep/#args","text":"resize : boolean (creates a subfolder of resized images, default False).","title":"Args"},{"location":"components/evaluation/","text":"class Evaluation Calculates performance metrics for trained models. Loads the best model (validation accuracy) from models directory in job directory. All metrics and graphs are based on test_samples.json in job directory. Plots will only be shown if number of classes 20 or less. Attributes image_dir : Path of image directory. job_dir : Path to job directory with samples. batch_size : Number of images per batch (default 64). base_model_name : Name of pretrained CNN (default MobileNet). __init__ def __init__ ( image_dir , job_dir , batch_size , base_model_name , ** kwargs ) Inits evaluation component. Loads the best model from job directory. Creates evaluation directory if app was started from commandline. get_correct_wrong_examples def get_correct_wrong_examples ( label ) Gets correctly and wrongly predicted samples for a given label. Args label : int or str (label for which the predictions should be considered). Returns (correct, wrong) : Tuple of two image lists. visualize_images def visualize_images ( image_list , title , show_heatmap , n_plot ) Visualizes images in a sample list. Args image_list : sample list. show_heatmap : boolean (generates a gradient based class activation map (grad-CAM), default False). n_plot : maximum number of plots to be shown (default 20). run def run ( report_create , report_kernel_name , report_export_html , report_export_pdf ) Runs evaluation pipeline on the best model found in job directory for the specific test set Makes prediction on test set Plots test set distribution Plots classification report (accuracy, precision, recall) Plots confusion matrix (on precsion and on recall) Plots correct and wrong examples If not in ipython mode an evaluation report is created. Args report_create : boolean (create ipython kernel) rt_kernel_name : str (name of ipython kernel) rt_export_html : boolean (exports report to html). rt_export_pdf : boolean (exports report to pdf).","title":"Evaluation"},{"location":"components/evaluation/#class-evaluation","text":"Calculates performance metrics for trained models. Loads the best model (validation accuracy) from models directory in job directory. All metrics and graphs are based on test_samples.json in job directory. Plots will only be shown if number of classes 20 or less.","title":"class Evaluation"},{"location":"components/evaluation/#attributes","text":"image_dir : Path of image directory. job_dir : Path to job directory with samples. batch_size : Number of images per batch (default 64). base_model_name : Name of pretrained CNN (default MobileNet).","title":"Attributes"},{"location":"components/evaluation/#9595init9595","text":"def __init__ ( image_dir , job_dir , batch_size , base_model_name , ** kwargs ) Inits evaluation component. Loads the best model from job directory. Creates evaluation directory if app was started from commandline.","title":"__init__"},{"location":"components/evaluation/#get95correct95wrong95examples","text":"def get_correct_wrong_examples ( label ) Gets correctly and wrongly predicted samples for a given label.","title":"get_correct_wrong_examples"},{"location":"components/evaluation/#args","text":"label : int or str (label for which the predictions should be considered).","title":"Args"},{"location":"components/evaluation/#returns","text":"(correct, wrong) : Tuple of two image lists.","title":"Returns"},{"location":"components/evaluation/#visualize95images","text":"def visualize_images ( image_list , title , show_heatmap , n_plot ) Visualizes images in a sample list.","title":"visualize_images"},{"location":"components/evaluation/#args_1","text":"image_list : sample list. show_heatmap : boolean (generates a gradient based class activation map (grad-CAM), default False). n_plot : maximum number of plots to be shown (default 20).","title":"Args"},{"location":"components/evaluation/#run","text":"def run ( report_create , report_kernel_name , report_export_html , report_export_pdf ) Runs evaluation pipeline on the best model found in job directory for the specific test set Makes prediction on test set Plots test set distribution Plots classification report (accuracy, precision, recall) Plots confusion matrix (on precsion and on recall) Plots correct and wrong examples If not in ipython mode an evaluation report is created.","title":"run"},{"location":"components/evaluation/#args_2","text":"report_create : boolean (create ipython kernel) rt_kernel_name : str (name of ipython kernel) rt_export_html : boolean (exports report to html). rt_export_pdf : boolean (exports report to pdf).","title":"Args"},{"location":"components/training/","text":"class Training Builds model and runs training. The following pretrained CNNs from Keras can be used for transfer learning: Xception VGG16 VGG19 ResNet50, ResNet101, ResNet152 ResNet50V2, ResNet101V2, ResNet152V2 ResNeXt50, ResNeXt101 InceptionV3 InceptionResNetV2 MobileNet MobileNetV2 DenseNet121, DenseNet169, DenseNet201 NASNetLarge, NASNetMobile Training is split into two phases, at first only the last dense layer gets trained, and then all layers are trained. The maximum number of epochs for each phase is set by epochs_train_dense (default: 100) and epochs_train_all (default: 100), respectively. Similarly, learning_rate_dense (default: 0.001) and learning_rate_all (default: 0.0003) can be set. For each phase the learning rate is reduced after a patience period if no improvement in validation accuracy has been observed. The patience period depends on the average number of samples per class: if n_per_class < 200: patience = 5 epochs if n_per_class >= 200 and < 500: patience = 4 epochs if n_per_class >= 500: patience = 2 epochs The training is stopped early after a patience period that is three times the learning rate patience to allow for two learning rate adjustments with no validation accuracy improvement before stopping training. Attributes image_dir : Directory with images used for training. job_dir : Directory with train_samples.json, val_samples.json, and class_mapping.json. epochs_train_dense : Maximum number of epochs to train dense layers (default 100). epochs_train_all : Maximum number of epochs to train all layers (default 100). learning_rate_dense : Learning rate for dense training phase (default 0.001). learning_rate_all : Learning rate for all training phase (default 0.0003). batch_size : Number of images per batch (default 64). dropout_rate : Fraction of nodes before output layer set to random value (default 0.75). base_model_name : Name of pretrained CNN (default MobileNet). __init__ def __init__ ( image_dir , job_dir , epochs_train_dense , epochs_train_all , learning_rate_dense , learning_rate_all , batch_size , dropout_rate , base_model_name , loss , ** kwargs ) Inits training component. Checks whether multiprocessing is available and sets number of workers for training. run def run () Builds the model and runs training.","title":"Training"},{"location":"components/training/#class-training","text":"Builds model and runs training. The following pretrained CNNs from Keras can be used for transfer learning: Xception VGG16 VGG19 ResNet50, ResNet101, ResNet152 ResNet50V2, ResNet101V2, ResNet152V2 ResNeXt50, ResNeXt101 InceptionV3 InceptionResNetV2 MobileNet MobileNetV2 DenseNet121, DenseNet169, DenseNet201 NASNetLarge, NASNetMobile Training is split into two phases, at first only the last dense layer gets trained, and then all layers are trained. The maximum number of epochs for each phase is set by epochs_train_dense (default: 100) and epochs_train_all (default: 100), respectively. Similarly, learning_rate_dense (default: 0.001) and learning_rate_all (default: 0.0003) can be set. For each phase the learning rate is reduced after a patience period if no improvement in validation accuracy has been observed. The patience period depends on the average number of samples per class: if n_per_class < 200: patience = 5 epochs if n_per_class >= 200 and < 500: patience = 4 epochs if n_per_class >= 500: patience = 2 epochs The training is stopped early after a patience period that is three times the learning rate patience to allow for two learning rate adjustments with no validation accuracy improvement before stopping training.","title":"class Training"},{"location":"components/training/#attributes","text":"image_dir : Directory with images used for training. job_dir : Directory with train_samples.json, val_samples.json, and class_mapping.json. epochs_train_dense : Maximum number of epochs to train dense layers (default 100). epochs_train_all : Maximum number of epochs to train all layers (default 100). learning_rate_dense : Learning rate for dense training phase (default 0.001). learning_rate_all : Learning rate for all training phase (default 0.0003). batch_size : Number of images per batch (default 64). dropout_rate : Fraction of nodes before output layer set to random value (default 0.75). base_model_name : Name of pretrained CNN (default MobileNet).","title":"Attributes"},{"location":"components/training/#9595init9595","text":"def __init__ ( image_dir , job_dir , epochs_train_dense , epochs_train_all , learning_rate_dense , learning_rate_all , batch_size , dropout_rate , base_model_name , loss , ** kwargs ) Inits training component. Checks whether multiprocessing is available and sets number of workers for training.","title":"__init__"},{"location":"components/training/#run","text":"def run () Builds the model and runs training.","title":"run"},{"location":"examples/cats_and_dogs/","text":"Cats and Dogs Example Install imageatm via PyPi pip install imageatm Download the cats and dogs dataset wget --no-check-certificate \\ https://storage.googleapis.com/mledu-datasets/cats_and_dogs_filtered.zip \\ -O cats_and_dogs_filtered.zip Unzip dataset and create working directory unzip cats_and_dogs_filtered.zip mkdir -p cats_and_dogs/train mv cats_and_dogs_filtered/train/cats/* cats_and_dogs/train mv cats_and_dogs_filtered/train/dogs/* cats_and_dogs/train Create the sample file import os import json filenames = os . listdir ( 'cats_and_dogs/train' ) sample_json = [] for i in filenames : sample_json . append ( { 'image_id' : i , 'label' : 'Cat' if 'cat' in i else 'Dog' } ) with open ( 'data.json' , 'w' ) as outfile : json . dump ( sample_json , outfile , indent = 4 , sort_keys = True ) Run the data preparation with resizing from imageatm.components import DataPrep dp = DataPrep ( image_dir = 'cats_and_dogs/train' , samples_file = 'data.json' , job_dir = 'cats_and_dogs' ) dp . run ( resize = True ) Initialize the Training class and run it from imageatm.components import Training trainer = Training ( dp . image_dir , dp . job_dir , epochs_train_dense = 5 , epochs_train_all = 5 ) trainer . run () Evaluate the best model from imageatm.components import Evaluation e = Evaluation ( image_dir = dp . image_dir , job_dir = dp . job_dir ) e . run () Visualize CAM analysis on the correct and wrong examples c , w = e . get_correct_wrong_examples ( label = 1 ) e . visualize_images ( w , show_heatmap = True ) e . visualize_images ( c , show_heatmap = True )","title":"Cats and Dogs"},{"location":"examples/cats_and_dogs/#cats-and-dogs-example","text":"","title":"Cats and Dogs Example"},{"location":"examples/cats_and_dogs/#install-imageatm-via-pypi","text":"pip install imageatm","title":"Install imageatm via PyPi"},{"location":"examples/cats_and_dogs/#download-the-cats-and-dogs-dataset","text":"wget --no-check-certificate \\ https://storage.googleapis.com/mledu-datasets/cats_and_dogs_filtered.zip \\ -O cats_and_dogs_filtered.zip","title":"Download the cats and dogs dataset"},{"location":"examples/cats_and_dogs/#unzip-dataset-and-create-working-directory","text":"unzip cats_and_dogs_filtered.zip mkdir -p cats_and_dogs/train mv cats_and_dogs_filtered/train/cats/* cats_and_dogs/train mv cats_and_dogs_filtered/train/dogs/* cats_and_dogs/train","title":"Unzip dataset and create working directory"},{"location":"examples/cats_and_dogs/#create-the-sample-file","text":"import os import json filenames = os . listdir ( 'cats_and_dogs/train' ) sample_json = [] for i in filenames : sample_json . append ( { 'image_id' : i , 'label' : 'Cat' if 'cat' in i else 'Dog' } ) with open ( 'data.json' , 'w' ) as outfile : json . dump ( sample_json , outfile , indent = 4 , sort_keys = True )","title":"Create the sample file"},{"location":"examples/cats_and_dogs/#run-the-data-preparation-with-resizing","text":"from imageatm.components import DataPrep dp = DataPrep ( image_dir = 'cats_and_dogs/train' , samples_file = 'data.json' , job_dir = 'cats_and_dogs' ) dp . run ( resize = True )","title":"Run the data preparation with resizing"},{"location":"examples/cats_and_dogs/#initialize-the-training-class-and-run-it","text":"from imageatm.components import Training trainer = Training ( dp . image_dir , dp . job_dir , epochs_train_dense = 5 , epochs_train_all = 5 ) trainer . run ()","title":"Initialize the Training class and run it"},{"location":"examples/cats_and_dogs/#evaluate-the-best-model","text":"from imageatm.components import Evaluation e = Evaluation ( image_dir = dp . image_dir , job_dir = dp . job_dir ) e . run ()","title":"Evaluate the best model"},{"location":"examples/cats_and_dogs/#visualize-cam-analysis-on-the-correct-and-wrong-examples","text":"c , w = e . get_correct_wrong_examples ( label = 1 ) e . visualize_images ( w , show_heatmap = True ) e . visualize_images ( c , show_heatmap = True )","title":"Visualize CAM analysis on the correct and wrong examples"},{"location":"examples/imagenette/","text":"Imagenette Example Install imageatm via PyPi pip install imageatm Download the Imagenette dataset (320px) and ImageNet mapping wget --no-check-certificate \\ https://s3.amazonaws.com/fast-ai-imageclas/imagenette-320.tgz wget --no-check-certificate \\ https://raw.githubusercontent.com/ozendelait/wordnet-to-json/master/mapping_imagenet.json Untar the dataset tar -xzf imagenette-320.tgz Create mapping for Imagenette classes and prepare the data.json import os import json def load_json ( file_path ): with open ( file_path , 'r' ) as f : return json . load ( f ) mapping = load_json ( 'mapping_imagenet.json' ) mapping_synset_txt = {} for i , j in enumerate ( mapping ): mapping_synset_txt [ j [ 'v3p0' ]] = j [ 'label' ] . split ( ',' )[ 0 ] classes = os . listdir ( 'imagenette-320/train' ) sample_json = [] for c in classes : filenames = os . listdir ( 'imagenette-320/train/ {} ' . format ( c )) for i in filenames : sample_json . append ( { 'image_id' : i , 'label' : mapping_synset_txt [ c ] } ) with open ( 'data.json' , 'w' ) as outfile : json . dump ( sample_json , outfile , indent = 4 , sort_keys = True ) Prepare our image directory IMAGE_DIR = 'images' if not os . path . exists ( IMAGE_DIR ): os . makedirs ( IMAGE_DIR ) classes = os . listdir ( 'imagenette-320/train' ) for c in classes : cmd = 'cp -r {} . {} ' . format ( os . path . join ( 'imagenette-320/train' , c ) + '/' , os . path . join ( IMAGE_DIR )) os . system ( cmd ) Run the data preparation from imageatm.components import DataPrep dp = DataPrep ( image_dir = 'images' , samples_file = 'data.json' , job_dir = 'imagenette' ) dp . run ( resize = False ) Initialize the Training class and run it from imageatm.components import Training trainer = Training ( dp . image_dir , dp . job_dir , epochs_train_dense = 5 , epochs_train_all = 5 , batch_size = 64 , ) trainer . run () Evaluate the best model from imageatm.components import Evaluation e = Evaluation ( image_dir = dp . image_dir , job_dir = dp . job_dir ) e . run () Visualize CAM analysis on the correct and wrong examples c , w = e . get_correct_wrong_examples ( label = 1 ) e . visualize_images ( w , show_heatmap = True ) e . visualize_images ( c , show_heatmap = True )","title":"Imagenette"},{"location":"examples/imagenette/#imagenette-example","text":"","title":"Imagenette Example"},{"location":"examples/imagenette/#install-imageatm-via-pypi","text":"pip install imageatm","title":"Install imageatm via PyPi"},{"location":"examples/imagenette/#download-the-imagenette-dataset-320px-and-imagenet-mapping","text":"wget --no-check-certificate \\ https://s3.amazonaws.com/fast-ai-imageclas/imagenette-320.tgz wget --no-check-certificate \\ https://raw.githubusercontent.com/ozendelait/wordnet-to-json/master/mapping_imagenet.json","title":"Download the Imagenette dataset (320px) and ImageNet mapping"},{"location":"examples/imagenette/#untar-the-dataset","text":"tar -xzf imagenette-320.tgz","title":"Untar the dataset"},{"location":"examples/imagenette/#create-mapping-for-imagenette-classes-and-prepare-the-datajson","text":"import os import json def load_json ( file_path ): with open ( file_path , 'r' ) as f : return json . load ( f ) mapping = load_json ( 'mapping_imagenet.json' ) mapping_synset_txt = {} for i , j in enumerate ( mapping ): mapping_synset_txt [ j [ 'v3p0' ]] = j [ 'label' ] . split ( ',' )[ 0 ] classes = os . listdir ( 'imagenette-320/train' ) sample_json = [] for c in classes : filenames = os . listdir ( 'imagenette-320/train/ {} ' . format ( c )) for i in filenames : sample_json . append ( { 'image_id' : i , 'label' : mapping_synset_txt [ c ] } ) with open ( 'data.json' , 'w' ) as outfile : json . dump ( sample_json , outfile , indent = 4 , sort_keys = True )","title":"Create mapping for Imagenette classes and prepare the data.json"},{"location":"examples/imagenette/#prepare-our-image-directory","text":"IMAGE_DIR = 'images' if not os . path . exists ( IMAGE_DIR ): os . makedirs ( IMAGE_DIR ) classes = os . listdir ( 'imagenette-320/train' ) for c in classes : cmd = 'cp -r {} . {} ' . format ( os . path . join ( 'imagenette-320/train' , c ) + '/' , os . path . join ( IMAGE_DIR )) os . system ( cmd )","title":"Prepare our image directory"},{"location":"examples/imagenette/#run-the-data-preparation","text":"from imageatm.components import DataPrep dp = DataPrep ( image_dir = 'images' , samples_file = 'data.json' , job_dir = 'imagenette' ) dp . run ( resize = False )","title":"Run the data preparation"},{"location":"examples/imagenette/#initialize-the-training-class-and-run-it","text":"from imageatm.components import Training trainer = Training ( dp . image_dir , dp . job_dir , epochs_train_dense = 5 , epochs_train_all = 5 , batch_size = 64 , ) trainer . run ()","title":"Initialize the Training class and run it"},{"location":"examples/imagenette/#evaluate-the-best-model","text":"from imageatm.components import Evaluation e = Evaluation ( image_dir = dp . image_dir , job_dir = dp . job_dir ) e . run ()","title":"Evaluate the best model"},{"location":"examples/imagenette/#visualize-cam-analysis-on-the-correct-and-wrong-examples","text":"c , w = e . get_correct_wrong_examples ( label = 1 ) e . visualize_images ( w , show_heatmap = True ) e . visualize_images ( c , show_heatmap = True )","title":"Visualize CAM analysis on the correct and wrong examples"},{"location":"handlers/data_generator/","text":"class DataGenerator Class inherits from Keras Sequence base object, allows to use multiprocessing in .fit_generator. DataGenerator is extended by these classes: TrainDataGenerator ValDataGenerator. Attributes samples : Dictionary of samples to generate data from. image_dir : Path of image directory. batch_size : Number of images per batch. n_classes : Number of classes in dataset. basenet_preprocess : Basenet specific preprocessing function. img_load_dims : Dimensions that images get resized into when loaded. train : If set to True samples are shuffled before each epoch and images are cropped once. __init__ def __init__ ( samples , image_dir , batch_size , n_classes , basenet_preprocess , img_load_dims , train ) Inits DataGenerator object. If train set True then samples are shuffled on init. on_epoch_end def on_epoch_end () Method called at the end of every epoch. If train set True then samples are shuffled. __len__ def __len__ () Number of batches in the Sequence. __getitem__ def __getitem__ ( index ) Gets batch at position index . If train set True then images will be cropped by img_crop_dims . class TrainDataGenerator Class inherits from DataGenerator. Per default images will be cropped and samples are shuffled before each epoch. Attributes samples : Dictionary of samples to generate data from. image_dir : Path of image directory. batch_size : Number of images per batch. n_classes : Number of classes in dataset. basenet_preprocess : Basenet specific preprocessing function. img_load_dims : Dimensions that images get resized into when loaded (default (256, 256)). img_crop_dims : Dimensions that images get resized into when loaded (default (224, 224)). train : If set to True samples are shuffled before each epoch and images are cropped once (default True). __init__ def __init__ ( samples , image_dir , batch_size , n_classes , basenet_preprocess , img_load_dims , img_crop_dims , train ) Inits TrainDataGenerator object. Per default samples are shuffled on init. class ValDataGenerator Class inherits from DataGenerator. Per default neither images are cropped nor samples are shuffled. Attributes samples : Dictionary of samples to generate data from. image_dir : Path of image directory. batch_size : Number of images per batch. n_classes : Number of classes in dataset. basenet_preprocess : Basenet specific preprocessing function. img_load_dims : Dimensions that images get resized into when loaded (default (224, 224)). train : If set to True samples are shuffled before each epoch and images are cropped once (default False). __init__ def __init__ ( samples , image_dir , batch_size , n_classes , basenet_preprocess , img_load_dims , train ) Inits TrainDataGenerator object.","title":"Data Generator"},{"location":"handlers/data_generator/#class-datagenerator","text":"Class inherits from Keras Sequence base object, allows to use multiprocessing in .fit_generator. DataGenerator is extended by these classes: TrainDataGenerator ValDataGenerator.","title":"class DataGenerator"},{"location":"handlers/data_generator/#attributes","text":"samples : Dictionary of samples to generate data from. image_dir : Path of image directory. batch_size : Number of images per batch. n_classes : Number of classes in dataset. basenet_preprocess : Basenet specific preprocessing function. img_load_dims : Dimensions that images get resized into when loaded. train : If set to True samples are shuffled before each epoch and images are cropped once.","title":"Attributes"},{"location":"handlers/data_generator/#9595init9595","text":"def __init__ ( samples , image_dir , batch_size , n_classes , basenet_preprocess , img_load_dims , train ) Inits DataGenerator object. If train set True then samples are shuffled on init.","title":"__init__"},{"location":"handlers/data_generator/#on95epoch95end","text":"def on_epoch_end () Method called at the end of every epoch. If train set True then samples are shuffled.","title":"on_epoch_end"},{"location":"handlers/data_generator/#9595len9595","text":"def __len__ () Number of batches in the Sequence.","title":"__len__"},{"location":"handlers/data_generator/#9595getitem9595","text":"def __getitem__ ( index ) Gets batch at position index . If train set True then images will be cropped by img_crop_dims .","title":"__getitem__"},{"location":"handlers/data_generator/#class-traindatagenerator","text":"Class inherits from DataGenerator. Per default images will be cropped and samples are shuffled before each epoch.","title":"class TrainDataGenerator"},{"location":"handlers/data_generator/#attributes_1","text":"samples : Dictionary of samples to generate data from. image_dir : Path of image directory. batch_size : Number of images per batch. n_classes : Number of classes in dataset. basenet_preprocess : Basenet specific preprocessing function. img_load_dims : Dimensions that images get resized into when loaded (default (256, 256)). img_crop_dims : Dimensions that images get resized into when loaded (default (224, 224)). train : If set to True samples are shuffled before each epoch and images are cropped once (default True).","title":"Attributes"},{"location":"handlers/data_generator/#9595init9595_1","text":"def __init__ ( samples , image_dir , batch_size , n_classes , basenet_preprocess , img_load_dims , img_crop_dims , train ) Inits TrainDataGenerator object. Per default samples are shuffled on init.","title":"__init__"},{"location":"handlers/data_generator/#class-valdatagenerator","text":"Class inherits from DataGenerator. Per default neither images are cropped nor samples are shuffled.","title":"class ValDataGenerator"},{"location":"handlers/data_generator/#attributes_2","text":"samples : Dictionary of samples to generate data from. image_dir : Path of image directory. batch_size : Number of images per batch. n_classes : Number of classes in dataset. basenet_preprocess : Basenet specific preprocessing function. img_load_dims : Dimensions that images get resized into when loaded (default (224, 224)). train : If set to True samples are shuffled before each epoch and images are cropped once (default False).","title":"Attributes"},{"location":"handlers/data_generator/#9595init9595_2","text":"def __init__ ( samples , image_dir , batch_size , n_classes , basenet_preprocess , img_load_dims , train ) Inits TrainDataGenerator object.","title":"__init__"},{"location":"handlers/image_classifier/","text":"class ImageClassifier Class that represents the classifier for images. The following pretrained CNNs from Keras can be used as base model: Xception VGG16 VGG19 ResNet50, ResNet101, ResNet152 ResNet50V2, ResNet101V2, ResNet152V2 ResNeXt50, ResNeXt101 InceptionV3 InceptionResNetV2 MobileNet MobileNetV2 DenseNet121, DenseNet169, DenseNet201 NASNetLarge, NASNetMobile Attributes base_model_name : Name of Keras base model. n_classes : Number of classes. learning_rate : Learning rate for training phase. dropout_rate : Fraction set randomly. loss : A loss function as one of two parameters to compile the model. weights : Pretrained weights the model architecture is loaded with (default imagenet). __init__ def __init__ ( base_model_name , n_classes , learning_rate , dropout_rate , loss , weights ) Inits ImageClassifier object. Loads Keras base_module specified by base_model_name. get_base_layers def get_base_layers () Gets layers of classifiers' base model Returns base_layers : list of layers get_preprocess_input def get_preprocess_input () Gets preprocess_input of classifiers' base_module Returns preprocess_input : Callable set_learning_rate def set_learning_rate ( learning_rate ) sets classifiers' learning_rate Args learning_rate : the learning_rate. build def build () Builds classifiers' model. The following steps will be performed in sequence: Loads a pretrained base model. Adds dropout and dense layer to the model. Sets classifiers' model. compile def compile () Configures classifiers' model for training. fit_generator def fit_generator ( ** kwargs ) Trains classifiers' model on data generated by a Python generator. Args generator : Input samples from a data generator on which to train the model. validation_data : Input samples from a data generator on which to evaluate the model. epochs : Number of epochs to train the model. initial_epoch : Epoch at which to start training. verbose : Verbosity mode. use_multiprocessing : Use process based threading. workers : Maximum number of processes. max_queue_size : Maximum size for the generator queue. callbacks : List of callbacks to apply during training. Returns history : A History object. predict_generator def predict_generator ( data_generator , ** kwargs ) Generates predictions for the input samples from a data generator. Args data_generator : Input samples from a data generator. workers : Maximum number of processes. use_multiprocessing : Use process based threading. verbose : Verbosity mode. Returns history : A History object. summary def summary () Summarizes classifiers' model.","title":"Image Classifer"},{"location":"handlers/image_classifier/#class-imageclassifier","text":"Class that represents the classifier for images. The following pretrained CNNs from Keras can be used as base model: Xception VGG16 VGG19 ResNet50, ResNet101, ResNet152 ResNet50V2, ResNet101V2, ResNet152V2 ResNeXt50, ResNeXt101 InceptionV3 InceptionResNetV2 MobileNet MobileNetV2 DenseNet121, DenseNet169, DenseNet201 NASNetLarge, NASNetMobile","title":"class ImageClassifier"},{"location":"handlers/image_classifier/#attributes","text":"base_model_name : Name of Keras base model. n_classes : Number of classes. learning_rate : Learning rate for training phase. dropout_rate : Fraction set randomly. loss : A loss function as one of two parameters to compile the model. weights : Pretrained weights the model architecture is loaded with (default imagenet).","title":"Attributes"},{"location":"handlers/image_classifier/#9595init9595","text":"def __init__ ( base_model_name , n_classes , learning_rate , dropout_rate , loss , weights ) Inits ImageClassifier object. Loads Keras base_module specified by base_model_name.","title":"__init__"},{"location":"handlers/image_classifier/#get95base95layers","text":"def get_base_layers () Gets layers of classifiers' base model","title":"get_base_layers"},{"location":"handlers/image_classifier/#returns","text":"base_layers : list of layers","title":"Returns"},{"location":"handlers/image_classifier/#get95preprocess95input","text":"def get_preprocess_input () Gets preprocess_input of classifiers' base_module","title":"get_preprocess_input"},{"location":"handlers/image_classifier/#returns_1","text":"preprocess_input : Callable","title":"Returns"},{"location":"handlers/image_classifier/#set95learning95rate","text":"def set_learning_rate ( learning_rate ) sets classifiers' learning_rate","title":"set_learning_rate"},{"location":"handlers/image_classifier/#args","text":"learning_rate : the learning_rate.","title":"Args"},{"location":"handlers/image_classifier/#build","text":"def build () Builds classifiers' model. The following steps will be performed in sequence: Loads a pretrained base model. Adds dropout and dense layer to the model. Sets classifiers' model.","title":"build"},{"location":"handlers/image_classifier/#compile","text":"def compile () Configures classifiers' model for training.","title":"compile"},{"location":"handlers/image_classifier/#fit95generator","text":"def fit_generator ( ** kwargs ) Trains classifiers' model on data generated by a Python generator.","title":"fit_generator"},{"location":"handlers/image_classifier/#args_1","text":"generator : Input samples from a data generator on which to train the model. validation_data : Input samples from a data generator on which to evaluate the model. epochs : Number of epochs to train the model. initial_epoch : Epoch at which to start training. verbose : Verbosity mode. use_multiprocessing : Use process based threading. workers : Maximum number of processes. max_queue_size : Maximum size for the generator queue. callbacks : List of callbacks to apply during training.","title":"Args"},{"location":"handlers/image_classifier/#returns_2","text":"history : A History object.","title":"Returns"},{"location":"handlers/image_classifier/#predict95generator","text":"def predict_generator ( data_generator , ** kwargs ) Generates predictions for the input samples from a data generator.","title":"predict_generator"},{"location":"handlers/image_classifier/#args_2","text":"data_generator : Input samples from a data generator. workers : Maximum number of processes. use_multiprocessing : Use process based threading. verbose : Verbosity mode.","title":"Args"},{"location":"handlers/image_classifier/#returns_3","text":"history : A History object.","title":"Returns"},{"location":"handlers/image_classifier/#summary","text":"def summary () Summarizes classifiers' model.","title":"summary"},{"location":"scripts/run_cloud/","text":"run_cloud def run_cloud ( provider , tf_dir , region , instance_type , vpc_id , bucket , destroy , job_dir , cloud_tag , ** kwargs )","title":"Run Cloud"},{"location":"scripts/run_cloud/#run95cloud","text":"def run_cloud ( provider , tf_dir , region , instance_type , vpc_id , bucket , destroy , job_dir , cloud_tag , ** kwargs )","title":"run_cloud"},{"location":"scripts/run_dataprep/","text":"run_dataprep def run_dataprep ( image_dir , samples_file , job_dir , resize , ** kwargs )","title":"Run Data Prep"},{"location":"scripts/run_dataprep/#run95dataprep","text":"def run_dataprep ( image_dir , samples_file , job_dir , resize , ** kwargs )","title":"run_dataprep"},{"location":"scripts/run_evaluation/","text":"run_evaluation def run_evaluation ( image_dir , job_dir , report , ** kwargs )","title":"Run Evaluation"},{"location":"scripts/run_evaluation/#run95evaluation","text":"def run_evaluation ( image_dir , job_dir , report , ** kwargs )","title":"run_evaluation"},{"location":"scripts/run_training/","text":"run_training def run_training ( image_dir , job_dir , ** kwargs )","title":"Run Training"},{"location":"scripts/run_training/#run95training","text":"def run_training ( image_dir , job_dir , ** kwargs )","title":"run_training"},{"location":"scripts/run_training_cloud/","text":"run_training_cloud def run_training_cloud ( image_dir , job_dir , provider , tf_dir , region , instance_type , vpc_id , bucket , destroy , cloud_tag , ** kwargs )","title":"Run Training Cloud"},{"location":"scripts/run_training_cloud/#run95training95cloud","text":"def run_training_cloud ( image_dir , job_dir , provider , tf_dir , region , instance_type , vpc_id , bucket , destroy , cloud_tag , ** kwargs )","title":"run_training_cloud"},{"location":"utils/images/","text":"load_image def load_image ( path , target_size ) save_image def save_image ( img , path ) validate_image def validate_image ( file_name , img_formats ) Checks whether File is valid image file: - file exists - file is readable - file is an image Args file_name : Absolute path of file. Returns resize_image def resize_image ( img , max_size , upscale ) Resizes image while keeping aspect ratio. The smaller dimension will be resized to max_size, i.e. a 400x500px image with max_size=300 will be resized to 300x375px. Args img : Pillow image object. max_size : Maximum width or height of resized image. upscale : If True will upscale small images to max_size. Returns resize_image_mp def resize_image_mp ( data_tuple ) random_crop def random_crop ( img , crop_dims )","title":"Images"},{"location":"utils/images/#load95image","text":"def load_image ( path , target_size )","title":"load_image"},{"location":"utils/images/#save95image","text":"def save_image ( img , path )","title":"save_image"},{"location":"utils/images/#validate95image","text":"def validate_image ( file_name , img_formats ) Checks whether File is valid image file: - file exists - file is readable - file is an image","title":"validate_image"},{"location":"utils/images/#args","text":"file_name : Absolute path of file.","title":"Args"},{"location":"utils/images/#returns","text":"","title":"Returns"},{"location":"utils/images/#resize95image","text":"def resize_image ( img , max_size , upscale ) Resizes image while keeping aspect ratio. The smaller dimension will be resized to max_size, i.e. a 400x500px image with max_size=300 will be resized to 300x375px.","title":"resize_image"},{"location":"utils/images/#args_1","text":"img : Pillow image object. max_size : Maximum width or height of resized image. upscale : If True will upscale small images to max_size.","title":"Args"},{"location":"utils/images/#returns_1","text":"","title":"Returns"},{"location":"utils/images/#resize95image95mp","text":"def resize_image_mp ( data_tuple )","title":"resize_image_mp"},{"location":"utils/images/#random95crop","text":"def random_crop ( img , crop_dims )","title":"random_crop"},{"location":"utils/io/","text":"load_json def load_json ( file_path ) save_json def save_json ( data , target_file ) load_yaml def load_yaml ( file_path )","title":"Io"},{"location":"utils/io/#load95json","text":"def load_json ( file_path )","title":"load_json"},{"location":"utils/io/#save95json","text":"def save_json ( data , target_file )","title":"save_json"},{"location":"utils/io/#load95yaml","text":"def load_yaml ( file_path )","title":"load_yaml"},{"location":"utils/logger/","text":"get_logger def get_logger ( name , job_dir )","title":"Logger"},{"location":"utils/logger/#get95logger","text":"def get_logger ( name , job_dir )","title":"get_logger"},{"location":"utils/process/","text":"parallelise def parallelise ( function , data ) run_cmd def run_cmd ( cmd , logger , level , return_output )","title":"Process"},{"location":"utils/process/#parallelise","text":"def parallelise ( function , data )","title":"parallelise"},{"location":"utils/process/#run95cmd","text":"def run_cmd ( cmd , logger , level , return_output )","title":"run_cmd"},{"location":"utils/tf_keras/","text":"load_model def load_model ( model_path ) use_multiprocessing def use_multiprocessing () _get_available_gpus def _get_available_gpus () class LoggingMetrics Callback for logging metrics at the end of each epoch. Args logger : Root logger. __init__ def __init__ ( logger ) on_epoch_end def on_epoch_end ( epoch , logs ) class LoggingModels __init__ def __init__ ( filepath , logger , monitor , verbose , save_best_only , save_weights_only , mode , period ) on_epoch_end def on_epoch_end ( epoch , logs )","title":"Tf keras"},{"location":"utils/tf_keras/#load95model","text":"def load_model ( model_path )","title":"load_model"},{"location":"utils/tf_keras/#use95multiprocessing","text":"def use_multiprocessing ()","title":"use_multiprocessing"},{"location":"utils/tf_keras/#95get95available95gpus","text":"def _get_available_gpus ()","title":"_get_available_gpus"},{"location":"utils/tf_keras/#class-loggingmetrics","text":"Callback for logging metrics at the end of each epoch.","title":"class LoggingMetrics"},{"location":"utils/tf_keras/#args","text":"logger : Root logger.","title":"Args"},{"location":"utils/tf_keras/#9595init9595","text":"def __init__ ( logger )","title":"__init__"},{"location":"utils/tf_keras/#on95epoch95end","text":"def on_epoch_end ( epoch , logs )","title":"on_epoch_end"},{"location":"utils/tf_keras/#class-loggingmodels","text":"","title":"class LoggingModels"},{"location":"utils/tf_keras/#9595init9595_1","text":"def __init__ ( filepath , logger , monitor , verbose , save_best_only , save_weights_only , mode , period )","title":"__init__"},{"location":"utils/tf_keras/#on95epoch95end_1","text":"def on_epoch_end ( epoch , logs )","title":"on_epoch_end"}]}